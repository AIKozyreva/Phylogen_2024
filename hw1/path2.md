#### Добаавляем материалы для Неандертальцев и Денисовцев, переделываем выравнивание и дерево. 

#### Потом создаём метаданные нужного формата и подгружаем всю красоту в ITOL.
```
План был хорош, но есть нюанс - у меня никогда ничего не идёт по плану. 
Начнём с создания выравниваний для материалов Неандертальцев и Денисовцев, а затем объеденим это с выравниванием, по которому строилось дерево в path1. 
```
__________________________________
#### Наскребаем метаданные для дерева из NCBI gbk файлов. 
В прошлой части мы слили вместе разные фасты из какого-то датасета (мы не знаем откуда они, но надеемся, что они все имеют стандартный фаста-заголовок с ncbi, у такого после знака `>` стоит ncbi-accession уникальный набор букв и числел). Мы эти все фасты сливали вместе, префращали в мультифасту, выравнивали записи в ней, получали таким образом мультивыравнивание, а потм по нему строили дерево и имели на ветках уникальные наборы цифро-букв, а именно ncbi-accessions. Чтобы поместить на дерево не цифро-буквы, а какие-то адекватные слова, имеющие значения, и тем самым сделать дерево читабельным - нам нужны метаданные о фаста-записях, которые мы использовали для построения выравнивания. Метаданные хранятся в базе ncbi и могут быть представлены в куче форматов. Иногда загружающие последовательность люди заполняют поля отдельно в установленном формате, тогда вы можете так или иначе попросить у базы ncbi только эти данные, а иногда данные нужно искать, потому что это нечто особенное (оно вносится в поле note, например, в свободном формате). Для особенной инфы нам нужно просмотреть глазами часто GBK файл (это то, что вы видите открывая какую-то запись на сайте ncbi). 

Для большого кол-ва образцов никто, конечно, смотреть GBK не будет - их массово скачивают, а потом парсят, пользуясь какими-либо текстовыми паттернами и/или регулярными выражениями, пытаясь достать нужную инфу. Вот именно это мы и сделаем в этом разделе через боль слёзы и я так ненавижу питон прости гсосподи, ну ладно R хуже. 

**Шаг 1 Как достать accessions NCBI из мультифасты**
Эти аксешн нуджны, зачем будет ясно в шаге2, но где то надо иметь их списком, поэтому будем их доставать. Открываем Python, загружаем в память вашу мультифасту или просто протягиваемся до неё (у мменя ноутбук питона на серве поднят, а файлы лежали там же), и пытаемся извлечь максимум пользы из заголовков каждой фаста-записи в мультифасте. 

```
import pandas as pd
from Bio import SeqIO

fasta_file = '/mnt/230924_phylo_hw/comb_part2_NDwmtHT.fasta'
data = []
for record in SeqIO.parse(fasta_file, "fasta"):
    header = record.description
    accession = header.split('Homo')[0]    
    if "haplogroup" in header:
        haplogroup = header.split("haplogroup")[1].split('mitochondrion')[0].strip()
    else:
        try:
            if "sapiens" in header:
                haplogroup = header.split('sapiens')[1].split('mitochondrion')[0].strip()
            else: 
                haplogroup = header.split('Homo')[1].split('from')[1].strip()
        except IndexError:
            haplogroup = "unknown"            
    data.append([accession, haplogroup])

haplo_meta_df = pd.DataFrame(data, columns=["accession", "haplogroup"])
haplo_meta_df.to_csv('/mnt/230924_phylo_hw/061024_part2/haplo_meta_df.tsv',sep="\t")
```
Всё - сохранили метаданные (ну как уж получилось) в табличку. Посмотрим чего там насохранялось. 


**Шаг 2 Как достать метаданные с NCBI**
Теория: у NCBI есть два варика программного доступа для массового или автоматического скачивания данных. 

Вариант 1: Это либо API (программный интерфейс) -> оно называется E-utils, оно работает как часть библиотеки Biopython, и я уже даже понимаю как оно работает (примерно), но лучше от этого не становится.

Вариант 2: Это пакет программ "ncbi-datasets-cli". Ставится через конду `conda install -c conda-forge ncbi-datasets-cli` и там есть программки datasets и dataformat - замечательные вещи, через которые удобно запрашивать большое кол-во типов данных и большое по объёму количество, НОО но только из баз genomes, gene, viruses NCBI. То есть доступ к случайной записи, которую вы через веб интерфейс находите в базе NCBI nucleotides (и там лежит метаинфа и сама например запись с частью какого-т гена, то как бы нет, он просто не пропустит запрос - выдаст ошибку bad request с кодом 400 - потому что такой базы он не знает).

мб недостаток Варианта2 - это специально, а мб они когда-то допилят доступ с этих тулов ко всей своей библиотеке, но пока для доступа к любому отделу NCBI, который не genome/gene/viruses - надо работать через E-utils.

Документация на E-utils NCBI: https://www.ncbi.nlm.nih.gov/books/NBK25498/#chapter3.ESearch__ESummaryEFetch (смотреть примеры использования Efetch Esummary); https://biopython.org/docs/1.76/api/Bio.Entrez.html 

Документация на ncbi-datasets-cli: https://www.ncbi.nlm.nih.gov/datasets/docs/v2/download-and-install/ 

Факт: для того, чтобы достать любые данные через любой из вариантов выше вам нужно указать какой-то идентификатор записи, что прога вообще должна спросить у базы. Чем больше параметров вы указываете, тем уже будет вопрос проги к базе - всё как если б вы выставляли фильтры ручками на сайте ncbi, но только происходит под капотом. В качестве таких идентификаторов могут выступать или accessions или ncbi taxon id. Обычно именно эти значения. 


